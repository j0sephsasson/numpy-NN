{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b6f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4baa36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.random.uniform(low=-2, high=5, size=(6,8))\n",
    "# ytrue = np.array([1,0,1,2,0,2])\n",
    "\n",
    "def get_data(path):\n",
    "    data = pd.read_csv(path, index_col=0)\n",
    "\n",
    "    cols = list(data.columns)\n",
    "    target = cols.pop()\n",
    "\n",
    "    X = data[cols].copy()\n",
    "    y = data[target].copy()\n",
    "\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = get_data(r'C:\\Users\\12482\\Desktop\\articles\\nn_from_scratch\\iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97337ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, neurons):\n",
    "        self.neurons = neurons\n",
    "        \n",
    "    def relu(self, inputs):\n",
    "        return np.maximum(0, inputs)\n",
    "\n",
    "    def softmax(self, inputs):\n",
    "        exp_scores = np.exp(inputs)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        return probs\n",
    "    \n",
    "    def relu_derivative(self, dA, Z):\n",
    "        dZ = np.array(dA, copy = True)\n",
    "        dZ[Z <= 0] = 0\n",
    "        return dZ\n",
    "    \n",
    "    def forward(self, inputs, weights, bias, activation):\n",
    "        Z_curr = np.dot(inputs, weights.T) + bias\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            A_curr = self.relu(inputs=Z_curr)\n",
    "        elif activation == 'softmax':\n",
    "            A_curr = self.softmax(inputs=Z_curr)\n",
    "            \n",
    "        return A_curr, Z_curr\n",
    "        \n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.network = [] ## layers\n",
    "        self.architecture = [] ## mapping input neurons --> output neurons\n",
    "        self.params = [] ## W, b\n",
    "        self.memory = [] ## Z, A\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.network.append(layer)\n",
    "            \n",
    "    def _compile(self, data):\n",
    "        for idx, layer in enumerate(self.network):\n",
    "            if idx == 0:\n",
    "                self.architecture.append({'input_dim':data.shape[1], \n",
    "                                          'output_dim':layer.neurons, 'activation':'relu'})\n",
    "            if idx == len(self.network)-2:\n",
    "                self.architecture.append({'input_dim':layer.neurons, \n",
    "                                          'output_dim':self.network[idx+1].neurons, 'activation':'softmax'})\n",
    "            elif idx != len(self.network)-2 and idx != len(self.network)-1:\n",
    "                self.architecture.append({'input_dim':layer.neurons, \n",
    "                                          'output_dim':self.network[idx+1].neurons, 'activation':'relu'})\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def _init_weights(self, data):\n",
    "        self._compile(data)\n",
    "        \n",
    "        for i in range(len(self.architecture)):\n",
    "            self.params.append({\n",
    "                'W':np.random.uniform(low=-1, high=1, \n",
    "                  size=(self.architecture[i]['output_dim'], \n",
    "                        self.architecture[i]['input_dim'])),\n",
    "                'b':np.zeros((1, self.architecture[i]['output_dim']))})\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _forwardprop(self, data):\n",
    "        self._init_weights(data)\n",
    "        \n",
    "        A_curr = data\n",
    "        \n",
    "        for i in range(len(self.params)):\n",
    "            A_prev = A_curr\n",
    "            A_curr, Z_curr = self.network[i].forward(inputs=A_prev, weights=self.params[i]['W'], \n",
    "                                           bias=self.params[i]['b'], activation=self.architecture[i]['activation'])\n",
    "            \n",
    "            self.memory.append({'inputs':A_prev, 'Z':Z_curr})\n",
    "            \n",
    "        return A_curr\n",
    "    \n",
    "    def _get_accuracy(self, predicted, actual):\n",
    "        return np.mean(np.argmax(predicted, axis=1)==actual)\n",
    "    \n",
    "    def _calculate_loss(self, predicted, actual):\n",
    "        samples = len(actual)\n",
    "\n",
    "        out_clipped = np.clip(predicted, 1e-7, 1-1e-7)\n",
    "\n",
    "        if len(actual.shape) == 1:\n",
    "            confs = out_clipped[range(samples), actual]\n",
    "        elif len(actual.shape) == 2:\n",
    "            confs = np.sum(out_clipped*actual, axis=1)\n",
    "\n",
    "        return np.mean(-np.log(confs))\n",
    "        \n",
    "\n",
    "model = Network()\n",
    "model.add(DenseLayer(X.shape[0]))\n",
    "model.add(DenseLayer(8))\n",
    "model.add(DenseLayer(10))\n",
    "model.add(DenseLayer(3))\n",
    "\n",
    "yhat = model._forwardprop(X)\n",
    "accuracy = model._get_accuracy(predicted=yhat, actual=y)\n",
    "loss = model._calculate_loss(predicted=yhat, actual=y)\n",
    "\n",
    "assert(len(yhat)==len(y))\n",
    "\n",
    "# print('PREDICTED:')\n",
    "# print(np.argmax(yhat, axis=1))\n",
    "\n",
    "# print()\n",
    "\n",
    "# print('ACTUAL:')\n",
    "# print(y)\n",
    "\n",
    "# print()\n",
    "\n",
    "# print('ACCURACY:', accuracy)\n",
    "# print('LOSS:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb9d73ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.146390893880092"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e675821d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.146390893880092"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = yhat / np.sum(yhat, axis=1, keepdims=True)\n",
    "correct_logprobs = -np.log(probs[range(150),y])\n",
    "data_loss = np.sum(correct_logprobs)/150\n",
    "\n",
    "data_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
