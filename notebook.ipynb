{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a9060c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "23e1f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(low=-2, high=5, size=(6,4))\n",
    "ytrue = np.array([1,0,1,2,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e58f8a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, neurons) -> None:\n",
    "        self.neurons = neurons\n",
    "        self.biases = np.zeros((1, neurons))\n",
    "        self.activation = None\n",
    "\n",
    "    def ReLU(self, inputs):\n",
    "        return np.maximum(0, inputs)\n",
    "    \n",
    "    def Softmax(self, inputs):\n",
    "        e_vals = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        return e_vals / np.sum(e_vals, axis=1, keepdims=True)\n",
    "    \n",
    "    def relu_derivative(self, dA, Z):\n",
    "        dZ = np.array(dA, copy = True)\n",
    "        dZ[Z <= 0] = 0;\n",
    "        return dZ;\n",
    "\n",
    "    def forward(self, inputs, last=False):\n",
    "        self.weights = np.random.uniform(low=-1, high=1, size=(inputs.shape[1], self.neurons))\n",
    "        if last == True:\n",
    "            self.Z = np.dot(inputs, self.weights) + self.biases\n",
    "            self.A = self.Softmax(self.Z)\n",
    "            self.activation = 'softmax'\n",
    "        else:\n",
    "            self.Z = np.dot(inputs, self.weights) + self.biases\n",
    "            self.A = self.ReLU(self.Z)\n",
    "            self.activation = 'relu'\n",
    "            \n",
    "    def backward(self, dA_curr, W_curr, Z_curr, A_prev):\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        dZ_curr = self.relu_derivative(dA_curr, Z_curr)\n",
    "        dW_curr = np.dot(dZ_curr.T, A_prev) / m\n",
    "        db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "        dA_prev = np.dot(W_curr, dZ_curr.T)\n",
    "\n",
    "        return dA_prev, dW_curr, db_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3d934822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, data):\n",
    "        self.network = []\n",
    "        self.memory = {}\n",
    "        self.gradients = {}\n",
    "        self.data = data\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.network.append(layer)\n",
    "    \n",
    "    def _one_hot(self, labels):\n",
    "        ohy = np.zeros((labels.size, labels.max() + 1))\n",
    "        ohy[np.arange(labels.size), labels] = 1\n",
    "        ohy_t = ohy.T\n",
    "        return ohy_t\n",
    "        \n",
    "    def _calculate_loss(self, outputs, labels):\n",
    "        samples = len(labels)\n",
    "\n",
    "        out_clipped = np.clip(outputs, 1e-7, 1-1e-7)\n",
    "\n",
    "        if len(labels.shape) == 1:\n",
    "            confs = out_clipped[range(samples), labels]\n",
    "        elif len(labels.shape) == 2:\n",
    "            confs = np.sum(out_clipped*labels, axis=1)\n",
    "\n",
    "        return np.mean(-np.log(confs))\n",
    "    \n",
    "    def _backprop(self, actual_y, predict_y):\n",
    "        actual_y = self._one_hot(labels=actual_y)\n",
    "        actual_y = actual_y.reshape(predict_y.shape)\n",
    "        \n",
    "        dA_prev = - (np.divide(actual_y, predict_y) - np.divide(1 - actual_y, 1 - predict_y))\n",
    "        \n",
    "        for layer_idx_prev, layer in reversed(list(enumerate(self.network))):\n",
    "            layer_idx_curr = layer_idx_prev + 1\n",
    "            \n",
    "            if layer_idx_prev == 0:\n",
    "                A_prev = self.data\n",
    "            else:\n",
    "                ## if output layer --> insert dC/y_hat, otherwise update with dC/dZ\n",
    "                dA_curr = dA_prev\n",
    "\n",
    "                A_prev = self.memory[layer_idx_prev]['A']\n",
    "                Z_curr = self.memory[layer_idx_curr]['Z']\n",
    "                W_curr = self.memory[layer_idx_curr]['W']\n",
    "            \n",
    "            dA_prev, dW_curr, db_curr = layer.backward(dA_curr, W_curr, Z_curr, A_prev)\n",
    "            \n",
    "            self.gradients[layer_idx_curr] = {'dW':dW_curr, 'db':db_curr}\n",
    "    \n",
    "    def _forwardprop(self):\n",
    "        new_out = []\n",
    "        for idx, layer in enumerate(self.network):\n",
    "            if layer != self.network[-1]:\n",
    "                if not new_out:\n",
    "                    layer.forward(self.data)\n",
    "                    new_out.append(layer.A)\n",
    "                    self.memory[idx+1] = {'W':layer.weights, 'Z':layer.Z, 'A':layer.A,\n",
    "                                         'b':layer.biases}\n",
    "                else:\n",
    "                    layer.forward(new_out[-1])\n",
    "                    new_out.append(layer.A)\n",
    "                    self.memory[idx+1] = {'W':layer.weights, 'Z':layer.Z, 'A':layer.A,\n",
    "                                         'b':layer.biases}\n",
    "            else:\n",
    "                layer.forward(new_out[-1], last=True)\n",
    "                new_out.append(layer.A)\n",
    "                self.memory[idx+1] = {'W':layer.weights, 'Z':layer.Z, 'A':layer.A,\n",
    "                                      'b':layer.biases}\n",
    "        \n",
    "        return new_out[-1]\n",
    "    \n",
    "    def _update(self, lr=0.01):\n",
    "        for idx, layer in enumerate(self.network):\n",
    "            idx = idx + 1\n",
    "            self.memory[idx]['W'] -= learning_rate * self.gradients[idx]['dW']     \n",
    "            self.memory[idx]['b'] -= learning_rate * self.gradients[idx]['db']\n",
    "            \n",
    "    def _get_accuracy(self, predicted, actual):\n",
    "        return np.mean(np.argmax(predicted, axis=1)==actual)\n",
    "            \n",
    "\n",
    "model = Network(data=X)\n",
    "model.add(DenseLayer(neurons=X.shape[0]))\n",
    "model.add(DenseLayer(neurons=6))\n",
    "model.add(DenseLayer(neurons=3))\n",
    "out = model._forwardprop()\n",
    "\n",
    "model._backprop(actual_y=ytrue, predict_y=out)\n",
    "\n",
    "# loss = model.calculate_loss(outputs=out, labels=ytrue)\n",
    "\n",
    "# print('PREDICTIONS:', np.argmax(out, axis=1))\n",
    "# print('ACTUAL:', ytrue)\n",
    "# print('LOSS:', loss)\n",
    "# print('ACCURACY:', np.mean(np.argmax(out, axis=1)==ytrue))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
